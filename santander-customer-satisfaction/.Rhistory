plot(housingFit)
text(housingFit, use.n=TRUE, all=TRUE, cex=.8)
treePredict <- predict(housingFit,newdata=housingTesting)
diff <- treePredict - housingTesting$MDEV
diff <- treePredict - housingTesting$MDEV
sumofsquares <- function(x) {return(sum(x^2))}
sumofsquares(diff)
adaModel <- ada(x=pimaTraining[,-9],y=pimaTraining$class,test.x=pimaTesting[,-9],test.y=pimaTesting$class)
library(ada)
adaModel <- ada(x=pimaTraining[,-9],y=pimaTraining$class,test.x=pimaTesting[,-9],test.y=pimaTesting$class)
adaModel
library("neuralnet")
nnet <- neuralnet(MDEV ~ CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PRATIO + B + LSTAT,housingTraining, hidden=10, threshold=0.01)
nnet <- neuralnet(MDEV ~ CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PRATIO + B + LSTAT,housingTraining)
plot(nnet, rep="best")
results <- compute(nnet, housingTesting[,-14])
diff <- results$net.result - housingTesting$MDEV
sumofsquares(diff)
library(randomForest)
forestFit <- randomForest(MDEV ~ CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PRATIO + B + LSTAT, data=housingTraining)
forestPredict <- predict(forestFit,newdata=housingTesting)
diff <- forestPredict - housingTesting$MDEV
sumofsquares(diff)
library(DMwR)
install.packages("DMwR")
data <- iris
nospecies <- data[,1:4]
scores <- lofactor(nospecies, k=3)
library(DMwR)
scores <- lofactor(nospecies, k=3)
plot(density(scores))
library(arules)
data <- read.csv("http://www.salemmarafi.com/wp-content/uploads/2014/03/groceries.csv")
rules <- apriori(data)
head(data)
data
view(data)
View(data)
rules
inspect(rules)
rules <- apriori(data, parameter = list(supp = 0.001, conf = 0.8))
rules
inspect(rules)
rules <- apriori(data)
rules
inspect(rules)
cov(housing)
plot(cov(housing))
cov(housing)
rm(list = ls())
setwd("~/git/github.com/kaggle-competitions/santander-customer-satisfaction")
# Script to load the Train and Test Dataset
source("src/utils/load_datasets.R")
# Explore Dataset
head(train,5)
head(test,5)
nrow(train)
nrow(test)
colnames(train)
colnames(test)
source("src/utils/clean_datasets.R")
head(train,5)
head(test,5)
nrow(train)
nrow(test)
colnames(train)
colnames(test)
library(sas7bdat)
library(caret)
library(gbm)
set.seed(1000)
fitControl <- trainControl(method = "repeatedcv",number = 4,repeats = 10)
y <- train$TARGET
y <- train$TARGET
set.seed(1000)
fitControl <- trainControl(method = "repeatedcv",number = 4,repeats = 10)
# Ejemplo grid para gbm
gbmGrid <-  expand.grid(interaction.depth = c(3,6),
n.trees = c(100),
shrinkage =c(0.1,0.02),
n.minobsinnode = c(5,10))
gbmFit1 <- train(factor(y) ~ ., data = train,
method = "gbm",trControl = fitControl,
verbose = FALSE,tuneGrid=gbmGrid)
gbmFit1
gbmGrid <-  expand.grid(interaction.depth = c(3),
n.trees = c(100),shrinkage =c(0.02),
n.minobsinnode = c(5))
gbmFit2 <- train(factor(y) ~ ., data = train,
method = "gbm",trControl = fitControl,
verbose = FALSE,tuneGrid=gbmGrid)
logisfit <- train(factor(y) ~ ., data = train,
method = "glm",trControl = fitControl)
logisfit
# **************************************
# EJEMPLO GRID RANDOM FOREST
# (SOLO PERMITE VARIAR MTRY)
# **************************************
# Conservo el anterior fitControl de validaci?n cruzada para comparar
# todos los modelos que voy creando
grid2 <- expand.grid(mtry=c(4,8,10))
# En esta ejecucion no pongo numero de muestra y utiliza
# por defecto todas las observaciones CON reemplazo
raforest <- train(factor(y) ~ ., data = train,
method = "rf",trControl = fitControl,maxnodes=50,
nodesize=10,ntree=100,tuneGrid=grid2)
raforest
# *********************************************************
# Ejemplo Grid con par?metros que no est?n considerados en caret
# Por ejemplo quiero variar maxnodes y mtry en random forest
# *********************************************************
# Creo datafin  y final archivo vac?o
datafin=data.frame(mtry=numeric(0),Accuracy=numeric(0),Kappa=numeric(0),
AccuracySD=numeric(0),KappaSD=numeric(0),nodes=numeric(0))
final=data.frame(Accuracy=numeric(0),Kappa=numeric(0), nodes=numeric(0),mtr=numeric(0))
# bucle para maxnodes y mtry
for(maxinodes in c(50,100))       {
for(mtr in c(4,10))      {
grid3 <- expand.grid(mtry=c(mtr))
raforest <- train(factor(y) ~ ., data = train,
method = "rf",trControl = fitControl,maxnodes=maxinodes,
nodesize=10,ntree=200,tuneGrid=grid3)
#  El elemento 4 de la lista raforest son los resultados globales
#  y el 14 las muestras
dat<-as.data.frame(raforest[[4]])
dat$nodes<-maxinodes
datafin<-rbind(datafin,dat)
ver<-as.data.frame(raforest[[14]])
ver<-ver[,-3]
ver$nodes<- maxinodes
ver$mtr<- mtr
final<-rbind(final,ver)
}}
boxplot(Accuracy ~ factor(nodes)+factor(mtr),data=final)
# Suponiendo que el mejor modelo Randomforest tiene 100 nodos, 4 variables
grid3 <- expand.grid(mtry=c(4))
raforest <- train(factor(y) ~ ., data = train,
method = "rf",trControl = fitControl,maxnodes=100,
nodesize=10,ntree=1000,tuneGrid=grid3)
spark_path <- strsplit(system("brew info apache-spark",intern=T)[4],' ')[[1]][1] # Get your spark path
.libPaths(c(file.path(spark_path,"libexec", "R", "lib"), .libPaths())) # Navigate to SparkR folder
library(SparkR) # Load the library
sparkR.stop()
sc <- sparkR.init(master="local")
dataset.test <- iris
sc <- sparkR.init()
sqlContext <- sparkRSQL.init(sc)
sparkr.df <- createDataFrame(sqlContext, dataset.test)
head(sparkr.df)
twitters_json <- read.df(sqlContext, "/Users/caiomsouza/git/github.com/TwitterRawData/examples/example-of-twitter-dataset.js", "json")
registerTempTable(twitters_json, "twitters")
test_twitter <- sql(sqlContext, "SELECT * FROM twitters")
head(test_twitter)
str(test_twitter)
sparkR.stop()
source("src/utils/load_datasets.R")
rm(list = ls())
setwd("~/git/github.com/kaggle-competitions/santander-customer-satisfaction")
source("src/utils/load_datasets.R")
train$ID <- NULL
summary(train)
library(caret)
data(iris)
summary(iris[,1:4])
preprocessParams <- preProcess(iris[,1:4], method=c("scale"))
print(preprocessParams)
transformed <- predict(preprocessParams, iris[,1:4])
summary(transformed)
library(mlbench)
library(caret)
data(PimaIndiansDiabetes)
summary(PimaIndiansDiabetes[,7:8])
preprocessParams <- preProcess(PimaIndiansDiabetes[,7:8], method=c("BoxCox"))
print(preprocessParams)
transformed <- predict(preprocessParams, PimaIndiansDiabetes[,7:8])
summary(transformed)
PimaIndiansDiabetes
PimaIndiansDiabetes[,7:8]
summary(train)
preprocessParams <- preProcess(train, method=c("BoxCox"))
print(preprocessParams)
transformed <- predict(preprocessParams, train)
summary(transformed)
train <- transformed
train$ID <- NULL
test.id <- test$ID
test$ID <- NULL
tc <- test
##### Extracting TARGET
train.y <- train$TARGET
train$TARGET <- NULL
##### 0 count per line
count0 <- function(x) {
return( sum(x == 0) )
}
train$n0 <- apply(train, 1, FUN=count0)
test$n0 <- apply(test, 1, FUN=count0)
##### Removing constant features
cat("\n## Removing the constants features.\n")
for (f in names(train)) {
if (length(unique(train[[f]])) == 1) {
cat(f, "is constant in train. We delete it.\n")
train[[f]] <- NULL
test[[f]] <- NULL
}
}
##### Removing identical features
features_pair <- combn(names(train), 2, simplify = F)
toRemove <- c()
for(pair in features_pair) {
f1 <- pair[1]
f2 <- pair[2]
if (!(f1 %in% toRemove) & !(f2 %in% toRemove)) {
if (all(train[[f1]] == train[[f2]])) {
cat(f1, "and", f2, "are equals.\n")
toRemove <- c(toRemove, f2)
}
}
}
feature.names <- setdiff(names(train), toRemove)
train$var38 <- log(train$var38)
test$var38 <- log(test$var38)
train <- train[, feature.names]
test <- test[, feature.names]
tc <- test
#---limit vars in test based on min and max vals of train
print('Setting min-max lims on test data')
for(f in colnames(train)){
lim <- min(train[,f])
test[test[,f]<lim,f] <- lim
lim <- max(train[,f])
test[test[,f]>lim,f] <- lim
}
#---
train$TARGET <- train.y
head(train)
nrow(train)
colnames(train)
train <- sparse.model.matrix(TARGET ~ ., data = train)
library(ggplot2) # Data visualization
library(readr) # CSV file I/O, e.g. the read_csv function
system("ls ../input")
library(xgboost)
library(Matrix)
set.seed(1234)
train <- sparse.model.matrix(TARGET ~ ., data = train)
dtrain <- xgb.DMatrix(data=train, label=train.y)
watchlist <- list(train=dtrain)
param <- list(  objective           = "binary:logistic",
booster             = "gbtree",
eval_metric         = "auc",
eta                 = 0.0202048,
max_depth           = 5,
subsample           = 0.6815,
colsample_bytree    = 0.701
)
clf <- xgb.train(   params              = param,
data                = dtrain,
nrounds             = 560,
print.every.n       = 10,
verbose             = 1,
watchlist           = watchlist,
maximize            = TRUE
)
feature.names
test$TARGET <- -1
test <- sparse.model.matrix(TARGET ~ ., data = test)
preds <- predict(clf, test)
pred <-predict(clf,train)
AUC<-function(actual,predicted)
{
library(pROC)
auc<-auc(as.numeric(actual),as.numeric(predicted))
auc
}
AUC(train.y,pred) ##AUC
nv = tc['num_var33']+tc['saldo_medio_var33_ult3']+tc['saldo_medio_var44_hace2']+tc['saldo_medio_var44_hace3']+
tc['saldo_medio_var33_ult1']+tc['saldo_medio_var44_ult1']
##TEST TRY num_var4 at top
#preds[tc['num_var4'] > 5] = 1
preds[nv > 0] = 0
preds[tc['var15'] < 23] = 0
preds[tc['saldo_medio_var5_hace2'] > 160000] = 0
preds[tc['saldo_var33'] > 0] = 0
preds[tc['var38'] > 3988596] = 0
preds[tc['var21'] > 7500] = 0
preds[tc['num_var30'] > 9] = 0
preds[tc['num_var33_0'] > 0] = 0
preds[tc['imp_op_var39_comer_ult3'] > 13184] = 0
preds[(tc['var15']+tc['num_var45_hace3']+tc['num_var45_ult3']+tc['var36']) <= 24] = 0
preds[tc['saldo_var8'] > 60099] = 0
preds[tc['saldo_var14'] > 19053.78] = 0
preds[tc['imp_ent_var16_ult1'] > 51003] = 0
preds[tc['saldo_var17'] > 288188.97] = 0
preds[tc['num_var13_largo_0'] > 3] = 0
preds[tc['saldo_medio_var13_largo_ult1'] > 0] = 0
preds[tc['num_var20_0'] > 0] = 0
# BAD
# saldo_var30 = tc['saldo_var30']
# num_var1 = tc['num_var1']
preds[tc['saldo_var5'] > 137615] = 0
# Testing
preds[tc['imp_op_var40_comer_ult1'] > 3639.87] = 0
preds[tc['saldo_var13_largo'] > 150000] = 0
# No improvement
preds[(tc['var15']+tc['num_var45_hace3']+tc['num_var45_ult3']+tc['var36']) <= 24] = 0
preds[preds<0.001]=0
preds[tc['num_meses_var13_largo_ult3'] > 0] = 0
preds[tc['saldo_medio_var5_ult3'] > 108251] = 0
# num_var35 = tc['num_var35']
preds[tc['saldo_var26'] > 10381.29] = 0
preds[tc['num_var13_0'] > 6] = 0
submission <- data.frame(ID=test.id, TARGET=preds)
head(preds)
head(submission)
cat("saving the submission file\n")
write.csv(submission, "outputs/predictions/xgb.model.1.BoxCox.csv", row.names = F)
test <- sparse.model.matrix(TARGET ~ ., data = test)
rm(list = ls())
setwd("~/git/github.com/kaggle-competitions/santander-customer-satisfaction")
# Script to load the Train and Test Dataset
source("src/utils/load_datasets.R")
summary(train)
library(mlbench)
library(caret)
# calculate the pre-process parameters from the dataset
preprocessParams <- preProcess(train, method=c("BoxCox"))
# summarize transform parameters
print(preprocessParams)
# transform the dataset using the parameters
transformed <- predict(preprocessParams, train)
# summarize the transformed dataset (note pedigree and age)
summary(transformed)
train <- transformed
summary(transformed)
summary(train)
train$ID <- NULL
test.id <- test$ID
test$ID <- NULL
tc <- test
##### Extracting TARGET
train.y <- train$TARGET
train$TARGET <- NULL
##### 0 count per line
count0 <- function(x) {
return( sum(x == 0) )
}
train$n0 <- apply(train, 1, FUN=count0)
test$n0 <- apply(test, 1, FUN=count0)
##### Removing constant features
cat("\n## Removing the constants features.\n")
for (f in names(train)) {
if (length(unique(train[[f]])) == 1) {
cat(f, "is constant in train. We delete it.\n")
train[[f]] <- NULL
test[[f]] <- NULL
}
}
##### Removing identical features
features_pair <- combn(names(train), 2, simplify = F)
toRemove <- c()
for(pair in features_pair) {
f1 <- pair[1]
f2 <- pair[2]
if (!(f1 %in% toRemove) & !(f2 %in% toRemove)) {
if (all(train[[f1]] == train[[f2]])) {
cat(f1, "and", f2, "are equals.\n")
toRemove <- c(toRemove, f2)
}
}
}
feature.names <- setdiff(names(train), toRemove)
train$var38 <- log(train$var38)
test$var38 <- log(test$var38)
train <- train[, feature.names]
test <- test[, feature.names]
tc <- test
#---limit vars in test based on min and max vals of train
print('Setting min-max lims on test data')
for(f in colnames(train)){
lim <- min(train[,f])
test[test[,f]<lim,f] <- lim
lim <- max(train[,f])
test[test[,f]>lim,f] <- lim
}
#---
train$TARGET <- train.y
head(train)
nrow(train)
colnames(train)
library(ggplot2) # Data visualization
library(readr) # CSV file I/O, e.g. the read_csv function
library(xgboost)
library(Matrix)
set.seed(1234)
train <- sparse.model.matrix(TARGET ~ ., data = train)
dtrain <- xgb.DMatrix(data=train, label=train.y)
watchlist <- list(train=dtrain)
param <- list(  objective           = "binary:logistic",
booster             = "gbtree",
eval_metric         = "auc",
eta                 = 0.0202048,
max_depth           = 5,
subsample           = 0.6815,
colsample_bytree    = 0.701
)
clf <- xgb.train(   params              = param,
data                = dtrain,
nrounds             = 560,
print.every.n       = 10,
verbose             = 1,
watchlist           = watchlist,
maximize            = TRUE
)
feature.names
test$TARGET <- -1
test <- sparse.model.matrix(TARGET ~ ., data = test)
preds <- predict(clf, test)
pred <-predict(clf,train)
AUC<-function(actual,predicted)
{
library(pROC)
auc<-auc(as.numeric(actual),as.numeric(predicted))
auc
}
AUC(train.y,pred) ##AUC
submission <- data.frame(ID=test.id, TARGET=preds)
head(preds)
head(submission)
write.csv(submission, "outputs/predictions/xgb.model.1.BoxCox.model.2016093-try-02.csv", row.names = F)
train <- train
bst <- xgboost(data = train$data, label = train$label, max.depth = 2,
eta = 1, nthread = 2, nround = 2,objective = "binary:logistic")
importance_matrix <- xgb.importance(train, model = bst)
rm(list = ls())
setwd("~/git/github.com/kaggle-competitions/santander-customer-satisfaction")
source("src/utils/load_datasets.R")
summary(train)
##### Removing IDs
train$ID <- NULL
test.id <- test$ID
test$ID <- NULL
tc <- test
##### Extracting TARGET
train.y <- train$TARGET
train$TARGET <- NULL
##### 0 count per line
count0 <- function(x) {
return( sum(x == 0) )
}
train$n0 <- apply(train, 1, FUN=count0)
test$n0 <- apply(test, 1, FUN=count0)
##### Removing constant features
cat("\n## Removing the constants features.\n")
for (f in names(train)) {
if (length(unique(train[[f]])) == 1) {
cat(f, "is constant in train. We delete it.\n")
train[[f]] <- NULL
test[[f]] <- NULL
}
}
##### Removing identical features
features_pair <- combn(names(train), 2, simplify = F)
toRemove <- c()
for(pair in features_pair) {
f1 <- pair[1]
f2 <- pair[2]
if (!(f1 %in% toRemove) & !(f2 %in% toRemove)) {
if (all(train[[f1]] == train[[f2]])) {
cat(f1, "and", f2, "are equals.\n")
toRemove <- c(toRemove, f2)
}
}
}
feature.names <- setdiff(names(train), toRemove)
train$var38 <- log(train$var38)
test$var38 <- log(test$var38)
train <- train[, feature.names]
test <- test[, feature.names]
tc <- test
#---limit vars in test based on min and max vals of train
print('Setting min-max lims on test data')
for(f in colnames(train)){
lim <- min(train[,f])
test[test[,f]<lim,f] <- lim
lim <- max(train[,f])
test[test[,f]>lim,f] <- lim
}
#---
train$TARGET <- train.y
head(train)
nrow(train)
colnames(train)
library(class)
library(dplyr)
library(lubridate)
set.seed(100)
normalize <- function(x) {
num <- x - min(x)
denom <- max(x) - min(x)
return (num/denom)
} - Read more at: http://scl.io/raimhAbo#gs.ZRsQk5w
normalize <- function(x) {
num <- x - min(x)
denom <- max(x) - min(x)
return (num/denom)
}
train.norm <- as.data.frame(lapply(train, normalize))
summary(train.norm)
iris_pred <- knn(train = train.norm, k=10)
iris_pred <- knn(train = train.norm, test = test, k=10)
iris_pred <- knn(train = train.norm, test = test, cl, k=10)
